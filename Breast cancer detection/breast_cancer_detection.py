# -*- coding: utf-8 -*-
"""Breast Cancer Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DvqT-uO-8WXgfe7ljv35eh5t1NJ4q6n7

# Part 1 :Data Preprocessing

dataset link: https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data

## Importing the lib and dataset
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

dataset =pd.read_csv('/content/data.csv')

dataset.head()

"""## Data Exploration"""

dataset.shape

dataset.info()

# Categorical data
dataset.select_dtypes(include='object').columns

dataset.select_dtypes(include='float64').columns

len(dataset.select_dtypes(include='object').columns)

# Numerical data
dataset.select_dtypes(include=['float64','int64']).columns

# Statistical summary
dataset.describe()

# To get the list of all the columns
dataset.columns

"""## Dealing with the missing values"""

# If there are any null value in this dataset
dataset.isnull().values.any()

dataset.isnull().values.sum()

# Columns with the null values
dataset.columns[dataset.isnull().any()]

len(dataset.columns[dataset.isnull().any()])

dataset['Unnamed: 32'].count()

"""This means that all the values in this column are null values"""

# Drop the column with null values
dataset = dataset.drop(columns='Unnamed: 32')

dataset.shape

# Again checking for null value sin our dataset
dataset.isnull().values.any()

"""## Dealing with categorical data"""

# Categorical data
dataset.select_dtypes(include='object').columns

# How many unique values in this column
dataset['diagnosis'].unique()

dataset['diagnosis'].nunique()

"""There are only two unique values malignant and benine"""

# one hot encoding
dataset = pd.get_dummies(data=dataset, drop_first=True).astype(int)

dataset.head()

"""## Countplot"""

dataset['diagnosis_M'] = dataset['diagnosis_M'].astype(int)

print(dataset['diagnosis_M'].unique())

plt.figure(figsize=(8, 6))
sns.countplot(x='diagnosis_M', data=dataset, label='Count')
plt.show()

# Total Benine values
(dataset.diagnosis_M== 0).sum()

# Total Maglignant values
(dataset.diagnosis_M== 1).sum()

"""## Correlation matrix and heatmap"""

# Diagonsis_M is the dependent variable so we have to drop that
dataset_2 = dataset.drop(columns='diagnosis_M')

dataset_2.head()

# Correlating the diagnosis_m with the rest data in the dataset
# to seet the correlation use corrwith function
dataset_2.corrwith(dataset['diagnosis_M']).plot.bar(
    figsize = (20,10), title = 'Correlated with diagnosis_M', rot = 45, grid = True
)

# Correlation matrix
corr = dataset.corr()

corr

# heatmap
plt.figure(figsize=(20,10))
sns.heatmap(corr, annot = True)

"""## Splitting the dataset into train and test set"""

dataset.head()

# dependent value y means the target varibale is diagnosis_m

# matrix of features / independent variables
# swlect all the columns except last one
x = dataset.iloc[:, 1:-1].values

x.shape

# Target variable / Dependent variable
# Only select the last column
y = dataset.iloc[:, -1].values

y.shape

"""There are total 569 rows"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= 0.2, random_state=0)

x_train.shape

x_test.shape

y_train.shape

y_test.shape

"""## Feature Scaling"""

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

x_train

x_test

"""# Part 2: Building the model

## Logistic Regression
"""

from sklearn.linear_model import LogisticRegression

classifir_lr = LogisticRegression(random_state=0)

classifir_lr.fit(x_train, y_train)

y_pred = classifir_lr.predict(x_test)

# Confusion matrix is used to see number of correct and number of incorrect predictions
# Accuracy score is to check accuracy of the model
# f1 score reaches best value at 1 and worst value at 0
# precision score tp is true positives and fp is false positives and its best score is also 1 and worst is 0
# recall score tp is true postive and fn is false negatives and best value is 1 and worst is 0

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score,precision_score,recall_score

acc = accuracy_score(y_test,y_pred)
f1 = f1_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)

results = pd.DataFrame([['Logistic Regression' , acc , f1, prec, rec]],
                       columns= ['Model', 'Accuracy', 'F1 Score', 'Precision' , 'Recall'])

results

"""Getting a nice accuracy"""

cm = confusion_matrix(y_test, y_pred)

cm

"""### Cross validation"""

from sklearn.model_selection import cross_val_score

accuracies = cross_val_score(estimator = classifir_lr, X = x_train, y= y_train, cv= 10)

print('Accuracy is {:.2f} %'.format(accuracies.mean()*100))
print('Standard Deviation is {:.2f} %'.format(accuracies.std()*100))

"""## Random Forest"""

from sklearn.ensemble import RandomForestClassifier

classifir_rf = RandomForestClassifier(random_state=0)
classifir_rf.fit(x_train, y_train)

y_pred = classifir_rf.predict(x_test)

from sklearn.metrics import accuracy_score, confusion_matrix, f1_score,precision_score,recall_score

acc = accuracy_score(y_test,y_pred)
f1 = f1_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)

model_results = pd.DataFrame([['Random Forest' , acc , f1, prec, rec]],
                       columns= ['Model', 'Accuracy', 'F1 Score', 'Precision' , 'Recall'])

model_results

cm = confusion_matrix(y_test, y_pred)
print(cm)

"""Total 5 incorrect predictions

### cross validation
"""

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifir_rf, X = x_train, y= y_train, cv= 10)
print('Accuracy is {:.2f} %'.format(accuracies.mean()*100))
print('Standard Deviation is {:.2f} %'.format(accuracies.std()*100))

"""# Part 3: Randomized Search to find the best parameters (Logistic Regression)"""

from sklearn.model_selection import RandomizedSearchCV

from scipy.stats import loguniform
from sklearn.model_selection import RandomizedSearchCV

# Adjust the param_distributions to make it valid
param_distributions = [
    {'solver': ['newton-cg', 'lbfgs'], 'penalty': ['l2'], 'C': loguniform(0.001, 100)},
    {'solver': ['saga'], 'penalty': ['l2', 'elasticnet'], 'C': loguniform(0.001, 100), 'l1_ratio': [0.15, 0.5, 0.85]},
    {'solver': ['liblinear'], 'penalty': ['l1', 'l2'], 'C': loguniform(0.001, 100)},
]

random_search = RandomizedSearchCV(estimator=classifir_lr, param_distributions=param_distributions,
                                   n_iter=10, scoring='roc_auc', n_jobs=-1, cv=10, verbose=3)

random_search.fit(x_train, y_train)

random_search.best_estimator_

random_search.best_score_

random_search.best_params_

"""# Part 4 : Final Model (Logistic Regression)"""

from sklearn.linear_model import LogisticRegression
classifir = LogisticRegression(C=1.1948445614769316, penalty='l1', random_state=0,
                   solver='liblinear')
classifir.fit(x_train, y_train)

y_pred = classifir.predict(x_test)

acc = accuracy_score(y_test,y_pred)
f1 = f1_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)

model_results = pd.DataFrame([['Final Logistic Regression' , acc , f1, prec, rec]],
                       columns= ['Model', 'Accuracy', 'F1 Score', 'Precision' , 'Recall'])

model_results

"""## Cross Validation"""

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifir, X = x_train, y= y_train, cv= 10)
print('Accuracy is {:.2f} %'.format(accuracies.mean()*100))
print('Standard Deviation is {:.2f} %'.format(accuracies.std()*100))

